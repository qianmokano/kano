# 感知机

## 什么是感知机？

感知机是一种最简单形式的神经网络，由美国学者*Frank Rosenblatt*于1957年提出。它是一个用于二分类问题的线性分类模型。

由于单层感知机的结构和功能都非常的简单，以至于目前在解决实际问题时很少被采用，但是由于它在神经网络研究中具有重要的意义，是研究其他深度网络的基础，所以理解单层感知机的原理是必要的。

## 单层感知机模型结构

感知机的模型如图：

![cd43f80c-ebf0-42d8-9e9e-66b67ad449a2](/tmp/cd43f80c-ebf0-42d8-9e9e-66b67ad449a2.png)

感知机的基本结构如下：

1. **输入层（感知层）：** 位于图中最左侧，也称为感知层，有n个神经元节点，这些节点只负责引入外部信息，自身不进行信息的处理。每个神经元节点接受一个输入信号。

2. **权重（*Weight*）：** 与每个输入特征相连的权重，用 \(w\) 表示。

3. **加权和（*Weighted Sum*）：** 对输入特征与对应权重的乘积进行求和，即
   $$
   z = \sum_{i=1}^{n} w_i \cdot x_i + b
   $$
   ，其中 \(n\) 是特征的数量，\(b\) 是偏置（bias）。

4. **激活函数（输出层）（*Activation Function*）：** 将加权和输入到激活函数中，激活函数根据阈值判断输出为1还是0。在最简单的感知机中，使用的是跃阶函数。

## 感知机的学习策略

假设训练数据集是线性可分的，感知机的学习目标就是确定一个能够将训练集正实例点和负实例点完全分离的超平面，也就是将训练集的输入正确地分类到+1和-1两个类别中。

感知机的输出 \(y\) 可以表示为：
$$
y = \begin{cases} 1, & \text{ } z \geq 0 \\ 0, & \text{ } z < 0 \end{cases}
$$
感知机的学习算法主要包括权重的调整，通过迭代更新权重以适应训练数据。一般来说，感知机的学习规则可以概括为：
$$
w_i = w_i + \eta \cdot (y_{\text{true}} - y_{\text{pred}}) \cdot x_i
$$

其中，
$$
y_{\text{true}}
$$
是样本的真实标签，
$$
y_{\text{pred}}
$$
是感知机的预测输出，
$$
\eta
$$
是学习率。

## 简单感知机的代码实现

```c++
#include <iostream>
#include <vector>
#include <cstdlib>
#include <ctime>
#include <random>
#include <algorithm>

using namespace std;

int predict(const vector<double>& data, const vector<double>& w, const double b);
double compute_loss(const vector<vector<double>>& Data, const vector<int>& Label, const vector<double>& w, const double b);

// 生成数据集
void generate_data(const int data_size, vector<vector<double>>& Data ,vector<int>& Label)
{
    random_device rd;
    mt19937 gen(rd());
    uniform_real_distribution<double> dis(-1.0, 1.0);

    for (int i = 0; i < data_size; ++i) 
    {
        vector<double> Random_num = {dis(gen), dis(gen)};
        Data.push_back(Random_num);
        
        // 计算点积
        double dot_product = Random_num[0] * 3 + Random_num[1] * 4 + 10;
        int label = dot_product > 0 ? 1 : -1; // 确定 label 
        Label.push_back(label); 
    } 
}

// 训练函数
void train(vector<vector<double>>& Data, vector<int>& Label, vector<double>& w, double& b, const int epochs, double learning_rate)
{
    for (size_t i = 0; i < epochs; i++)
    {
        for (size_t j = 0; j < Data.size(); j++)
        {
            int predict_label = predict(Data[j], w, b);
            if (predict_label != Label[j])
            {
                for (size_t k = 0; k < 2; k++)
                {
                    w[k] += learning_rate * Label[j] * Data[j][k];
                }
                b += learning_rate * Label[j];
            }
        }
        
    }
}



// 预测函数
int predict(const vector<double>& data, const vector<double>& w, const double b)
{
    double dot_product = 0;
    for (size_t i = 0; i < data.size(); i++)
    {
        dot_product += data[i] * w[i];
    }
    double predict_result = dot_product + b;
    return predict_result > 0 ? 1 : -1;
}

int main()
{
    // 生成数据集
    vector<vector<double>> Data;
    vector<int> Label;
    int data_size = 1000; // 数据集大小
    generate_data(data_size, Data, Label);

    // 初始化权重偏置
    vector<double> w = {0, 0};
    double b = 0;

    // 设定学习率迭代次数
    double learning_rate = 0.01; // 学习率
    int epochs = 100; // 迭代次数

    // 开始训练
    train(Data, Label, w, b, epochs, learning_rate);

    cout << "训练结果: w: " << w[0] << ", " << w[1]  << ", b: "<< b << endl;

    // 测试
    vector<vector<double>> Test;
    vector<int> Test_Label;
    int test_size = 1; 
    generate_data(test_size, Test, Test_Label);
    int Predict_Label = predict(Test[0], w, b);
    cout << "测试数据：" << Test[0][0] << ", " << Test[0][1] << endl;
    cout << "真实标签：" << Test_Label[0] << endl;
    cout << "预测标签：" << Predict_Label << endl;

    return 0;
}
```
